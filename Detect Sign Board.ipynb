{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5.git\n",
    "!pip install -r yolov5/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include all packages\n",
    "import os\n",
    "import cv2\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from yolov5.models.yolo import Model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import zipfile\n",
    "with zipfile.ZipFile('/content/drive/MyDrive/DL Project/DataSet.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ResizeImage(image: np.ndarray, x1: int, y1: int, x2: int, y2: int, newWidth: int, newHeight: int) -> tuple:\n",
    "    originalHeight, originalWidth = image.shape[:2]\n",
    "    widthScale = newWidth / originalWidth\n",
    "    heightScale = newHeight / originalHeight\n",
    "    resizedImage = cv2.resize(\n",
    "        image, (newWidth, newHeight), interpolation=cv2.INTER_LINEAR)\n",
    "    x1New, y1New = int(x1 * widthScale), int(y1 * heightScale)\n",
    "    x2New, y2New = int(x2 * widthScale), int(y2 * heightScale)\n",
    "    return resizedImage, x1New, y1New, x2New, y2New\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadDataSet(dataSetFolderPath: str) -> tuple:\n",
    "    images = []\n",
    "    annotations = []\n",
    "    annotationsFilePath = dataSetFolderPath+\"/allAnnotations.csv\"\n",
    "    annotationsDataFrame = pd.read_csv(annotationsFilePath, sep=\";\")\n",
    "    uniqueSigns = annotationsDataFrame['Annotation tag'].unique().tolist()\n",
    "    for index, row in annotationsDataFrame[1:].iterrows():\n",
    "        image = cv2.imread(dataSetFolderPath+\"/\"+row[0])\n",
    "        images.append(image)\n",
    "        annotations.append(\n",
    "            [uniqueSigns.index(row[1]), row[2], row[3], row[4], row[5]])\n",
    "\n",
    "    del annotationsDataFrame\n",
    "\n",
    "    return images, annotations, len(uniqueSigns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessDataSet(images: list, annotations: list, batchSize: int, resize: tuple) -> tuple:\n",
    "    resizedImages = []\n",
    "    newAnnotations = []\n",
    "    for i, image in enumerate(images):\n",
    "        [label, x1, y1, x2, y2] = annotations[i]\n",
    "        resizedImage, x1New, y1New, x2New, y2New = ResizeImage(\n",
    "            image, x1, y1, x2, y2, resize[0], resize[1])\n",
    "        resizedImages.append(resizedImage)\n",
    "        newAnnotations.append(\n",
    "            [(i % batchSize), label, x1New, y1New, x2New, y2New])\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        resizedImages, newAnnotations, test_size=0.3, random_state=42)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputData, label = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            inputData = self.transform(inputData)\n",
    "        inputData = torch.from_numpy(inputData).float()\n",
    "        label = torch.tensor(label).float()\n",
    "        return inputData, label\n",
    "\n",
    "def CreateDataLoaders(X_train, X_val, y_train, y_val, batchSize):\n",
    "    trainDataSet = []\n",
    "    valDataSet = []\n",
    "    for i in range(len(X_train)):\n",
    "        trainDataSet.append((X_train[i], y_train[i]))\n",
    "\n",
    "    for i in range(len(X_val)):\n",
    "        valDataSet.append((X_val[i], y_val[i]))\n",
    "\n",
    "    trainDataSet = CustomDataset(trainDataSet)\n",
    "    valDataSet = CustomDataset(valDataSet)\n",
    "    trainDataLoader = DataLoader(\n",
    "        trainDataSet, batch_size=batchSize, shuffle=True, num_workers=4)\n",
    "    valDataLoader = DataLoader(\n",
    "        valDataSet, batch_size=batchSize, shuffle=False, num_workers=4)\n",
    "\n",
    "    return trainDataLoader, valDataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TargetstoTensors(targets, batch_size, numAnchors, gridSizes, numClasses):\n",
    "    targetObj = []\n",
    "    targetClass = []\n",
    "    targetBox = []\n",
    "    # batch_size = 32\n",
    "    for grid_size in gridSizes:\n",
    "        targetObj.append(torch.zeros(\n",
    "            (batch_size, numAnchors, grid_size, grid_size, 1)))\n",
    "        targetClass.append(torch.zeros(\n",
    "            (batch_size, numAnchors, grid_size, grid_size, numClasses)))\n",
    "        targetBox.append(torch.zeros(\n",
    "            (batch_size, numAnchors, grid_size, grid_size, 4)))\n",
    "    # current_batch_size = int(targets[:, 0].max()) + 1\n",
    "    # print(\"current_batch_size\",targets.size(0))\n",
    "    for target in targets:\n",
    "        batchIndex, cls, xCenter, yCenter, width, height = target.long()\n",
    "\n",
    "        for i, grid_size in enumerate(gridSizes):\n",
    "\n",
    "            x_cell, y_cell = int(\n",
    "                xCenter * grid_size), int(yCenter * grid_size)\n",
    "            anchor = 0\n",
    "            try:\n",
    "                targetObj[i][batchIndex, anchor, y_cell, x_cell, 0] = 1\n",
    "                targetClass[i][batchIndex, anchor, y_cell, x_cell, cls] = 1\n",
    "                targetBox[i][batchIndex, anchor, y_cell, x_cell] = torch.tensor(\n",
    "                    [xCenter, yCenter, width, height])\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                pass\n",
    "    return targetObj, targetClass, targetBox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv5Loss(nn.Module):\n",
    "    def __init__(self, numClasses, numAnchors=3):\n",
    "        super(YOLOv5Loss, self).__init__()\n",
    "        self.numClasses = numClasses\n",
    "        self.numAnchors = numAnchors\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        objectLoss = torch.tensor(0.0, device=preds[0].device)\n",
    "        classLoss = torch.tensor(0.0, device=preds[0].device)\n",
    "        boxLoss = torch.tensor(0.0, device=preds[0].device)\n",
    "        batch_size = preds[0].size(0)\n",
    "        gridSizes = [pred.size(2) for pred in preds]\n",
    "        targetObjList, targetClassList, targetBoxList = TargetstoTensors(\n",
    "            targets, batch_size, self.numAnchors, gridSizes, self.numClasses)\n",
    "        for i, pred in enumerate(preds):\n",
    "            # pred_obj = pred[..., 4].sigmoid()\n",
    "            # pred_cls = pred[..., 5:].sigmoid()\n",
    "            # pred_box = pred[..., :4]\n",
    "\n",
    "            targetObj = targetObjList[i].to(pred.device)\n",
    "            targetClass = targetClassList[i].to(pred.device)\n",
    "            targetBox = targetBoxList[i].to(pred.device)\n",
    "\n",
    "            objectLoss += nn.BCEWithLogitsLoss()(pred[..., 4:5], targetObj)\n",
    "            classLoss += nn.BCEWithLogitsLoss()(pred[..., 5:], targetClass)\n",
    "            boxLoss += nn.MSELoss()(pred[..., :4], targetBox)\n",
    "\n",
    "        totalLoss = objectLoss + classLoss + boxLoss\n",
    "        return totalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateYolov5Model(numClasses: int, version=\"s\"):\n",
    "    congfigFile = \"yolov5/models/yolov5{}.yaml\".format(version)\n",
    "    model = Model(congfigFile, ch=3, nc=numClasses)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image, input_shape, device, conf_thres=0.5, iou_thres=0.5):\n",
    "    model.eval()\n",
    "    image = cv2.resize(image, input_shape, interpolation=cv2.INTER_LINEAR)\n",
    "    img_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        output = torchvision.ops.nms(output[0], conf_thres, iou_thres)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "inputShape = (416, 416)\n",
    "epochs = 100\n",
    "numAnchors = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, annotations, numClasses = LoadDataSet(\"./DataSet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = PreProcessDataSet(\n",
    "    images, annotations, batchSize, inputShape)\n",
    "del images\n",
    "del annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader, valDataLoader = CreateDataLoaders(\n",
    "    X_train, X_val, y_train, y_val, batchSize)\n",
    "del X_train\n",
    "del y_train\n",
    "del X_val\n",
    "del y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5Model = CreateYolov5Model(numClasses)\n",
    "optimizer = optim.Adam(yolov5Model.parameters(), lr=0.001)\n",
    "yolov5LossFunction= YOLOv5Loss(numClasses=numClasses)\n",
    "yolov5Model = yolov5Model.to(device)\n",
    "yolov5LossFunction = yolov5LossFunction.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5Model.load_state_dict(torch.load('signPredictionV1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_image(yolov5Model, input_image, inputShape, device)\n",
    "for pred in predictions:\n",
    "    label, x1, y1, x2, y2 = pred\n",
    "    x, y, width, height = int(x1), int(y1), int(x2 - x1), int(y2 - y1)\n",
    "    print(\"Label: {}, X: {}, Y: {}, Width: {}, Height: {}\".format(label, x, y, width, height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
