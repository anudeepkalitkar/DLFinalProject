{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NLuliN1UcKJ7","outputId":"12abff3b-2fa6-4578-c1eb-62178ad1b173"},"outputs":[],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import zipfile\n","    with zipfile.ZipFile('/content/drive/MyDrive/DL Project/DataSet1.zip', 'r') as zip_ref:\n","        zip_ref.extractall('./DataSet1')\n","except:\n","    print(\"Using Local Machine\")\n","!git clone https: // github.com/ultralytics/yolov5.git\n","!pip install - r yolov5/requirements.txt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQdle6ECbu-5"},"outputs":[],"source":["# Include all packages\n","import gc\n","import cv2\n","import shutil\n","import random\n","import numpy as np\n","import pandas as pd\n","from time import time\n","from copy import deepcopy\n","\n","from datetime import datetime\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","from yolov5.models.yolo import Model\n","from sklearn.model_selection import train_test_split\n","\n","import torchvision\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxOLsWl2cKJ8"},"outputs":[],"source":["def CannyEdge(capturedImage):\n","    grayScale = cv2.cvtColor(capturedImage, cv2.COLOR_BGR2GRAY)\n","    constrastKernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n","    topHat = cv2.morphologyEx(grayScale, cv2.MORPH_TOPHAT, constrastKernel)\n","    blackHat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, constrastKernel)\n","    grayScale = grayScale + topHat - blackHat\n","    gaussianImage = cv2.GaussianBlur(grayScale, (3, 3), 0)\n","    imageMedian = np.median(capturedImage)\n","    lowerThreshold = max(0, (0.7 * imageMedian))\n","    upperThreshold = min(255, (0.7 * imageMedian))\n","    cannyEdgeImage = cv2.Canny(gaussianImage, lowerThreshold, upperThreshold)\n","    cannyEdgeImage = cv2.bitwise_not(cannyEdgeImage)\n","    cannyEdgeImage = cv2.cvtColor(cannyEdgeImage, cv2.COLOR_GRAY2BGR)\n","    return cannyEdgeImage\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xd2tU8GB3nB3"},"outputs":[],"source":["def ResizeImage(image: np.ndarray, x1: int, y1: int, x2: int, y2: int, newWidth: int, newHeight: int) -> tuple:\n","    originalHeight, originalWidth = image.shape[:2]\n","    resizedImage = cv2.resize(\n","        image, (newWidth, newHeight), interpolation=cv2.INTER_LINEAR)\n","    widthScale = newWidth / originalWidth\n","    heightScale = newHeight / originalHeight\n","    x1New, y1New = int(x1 * widthScale), int(y1 * heightScale)\n","    x2New, y2New = int(x2 * widthScale), int(y2 * heightScale)\n","    return resizedImage, x1New, y1New, x2New, y2New\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iMxCddibu-7"},"outputs":[],"source":["def LoadDataSet(dataSetFolderPath: str) -> tuple:\n","    images = []\n","    annotations = []\n","    resize = (640, 640)\n","    annotationsFilePath = dataSetFolderPath+\"/annotations.csv\"\n","    annotationsDataFrame = pd.read_csv(annotationsFilePath, sep=\",\")\n","    uniqueSigns = annotationsDataFrame['class'].unique().tolist()\n","    for index, row in annotationsDataFrame[1:].iterrows():\n","        image = cv2.imread(dataSetFolderPath+\"/\"+row[0])\n","        [classIndex, x1, y1, x2, y2] = [uniqueSigns.index(row[5]), row[1], row[2], row[3], row[4]]\n","        resizedImage, x1New, y1New, x2New, y2New = ResizeImage(image, x1, y1, x2, y2, resize[0], resize[1])\n","        # resizedImage = CannyEdge(resizedImage)\n","        images.append(resizedImage)\n","        annotations.append(\n","            [classIndex, x1New, y1New, x2New, y2New])\n","    del annotationsDataFrame\n","\n","    X_train, X_val, y_train, y_val = train_test_split(images, annotations, test_size=0.2, random_state=42)\n","\n","    return len(uniqueSigns), X_train, X_val, y_train, y_val\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68sCzcEAbu-7"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data, transform=None):\n","        self.data = data\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        inputData, label = self.data[idx]\n","\n","        if self.transform:\n","            inputData = self.transform(inputData)\n","        inputData = torch.from_numpy(inputData).float()\n","        label = torch.tensor(label).float()\n","        return inputData, label\n","\n","\n","def CreateDataLoaders(X_train, X_val, y_train, y_val, batchSize):\n","    trainDataSet = []\n","    valDataSet = []\n","    for i in range(len(X_train)):\n","        trainDataSet.append((X_train[i], y_train[i]))\n","\n","    for i in range(len(X_val)):\n","        valDataSet.append((X_val[i], y_val[i]))\n","\n","    trainDataSet = CustomDataset(trainDataSet)\n","    valDataSet = CustomDataset(valDataSet)\n","    trainDataLoader = DataLoader(\n","        trainDataSet, batch_size=batchSize, shuffle=True, num_workers=4)\n","    valDataLoader = DataLoader(\n","        valDataSet, batch_size=batchSize, shuffle=False, num_workers=4)\n","\n","    return trainDataLoader, valDataLoader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owhzzQF61a0L"},"outputs":[],"source":["\n","def TargetstoTensors(targets, batchSize, numAnchors, gridSizes, numClasses):\n","    targetObj = []\n","    targetBox = []\n","    targetClass = []\n","    for grid_size in gridSizes:\n","        targetObj.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, 1)))\n","        targetBox.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, 4)))\n","        targetClass.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, numClasses)))\n","\n","    for batch_index, target in enumerate(targets):\n","        classindex, x1, y1, x2, y2 = target\n","        x_center, y_center, width, height = (x1 + x2) / 2, (y1 + y2) / 2, x2 - x1, y2 - y1\n","\n","        for i, grid_size in enumerate(gridSizes):\n","            x_cell, y_cell = int(x_center * grid_size), int(y_center * grid_size)\n","            anchor = 0\n","            try:\n","                targetObj[i][batch_index, anchor, y_cell, x_cell, 0] = 1\n","                targetBox[i][batch_index, anchor, y_cell, x_cell] = torch.tensor([x_center, y_center, width, height])\n","                targetClass[i][batch_index, anchor, y_cell, x_cell, classindex] = 1\n","            except Exception as e:\n","                pass\n","    return targetObj, targetBox, targetClass\n","\n","\n","class SignboardLoss(nn.Module):\n","    def __init__(self, num_anchors=3, num_classes=0):\n","        super(SignboardLoss, self).__init__()\n","        self.num_anchors = num_anchors\n","        self.num_classes = num_classes\n","\n","    def forward(self, preds, targets):\n","        objectLoss = torch.tensor(0.0, device=preds[0].device)\n","        boxLoss = torch.tensor(0.0, device=preds[0].device)\n","        classLoss = torch.tensor(0.0, device=preds[0].device)\n","        batchSize = preds[0].size(0)\n","        gridSizes = [pred.size(2) for pred in preds]\n","        targetObjList, targetBoxList, targetClassList = TargetstoTensors(targets, batchSize, self.num_anchors, gridSizes, self.num_classes)\n","\n","        for i, pred in enumerate(preds):\n","            targetObj = targetObjList[i].to(pred.device)\n","            targetBox = targetBoxList[i].to(pred.device)\n","            targetClass = targetClassList[i].to(pred.device)\n","\n","            objectLoss += nn.BCEWithLogitsLoss()(pred[..., 4:5], targetObj)\n","            boxLoss += nn.MSELoss()(pred[..., :4], targetBox)\n","            classLoss += nn.BCEWithLogitsLoss()(pred[..., 5:], targetClass)\n","\n","        total_loss = objectLoss + boxLoss + classLoss\n","        return total_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZJ8-pMgbu-9"},"outputs":[],"source":["def CreateYolov5Model(numClasses: int, version: str, device):\n","    congfigFile = \"yolov5/models/yolov5{}.yaml\".format(version)\n","    model = Model(congfigFile, ch=3, nc=numClasses)\n","    ckpt = torch.load(f'yolov5{version}.pt', map_location=device)\n","    ckpt_model_dict = ckpt['model'].state_dict()\n","    compatible_weights = {k: v for k, v in ckpt_model_dict.items(\n","    ) if k in model.state_dict() and model.state_dict()[k].shape == v.shape}\n","    model.load_state_dict(compatible_weights, strict=False)\n","    model.hyp = ckpt['model'].hyp\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWVv6VcgcKJ-"},"outputs":[],"source":["def TrainEpoch(model, dataLoader, optimizer, lossFunction, device):\n","    print(\"Training Epoch\")\n","    model.train()\n","    runningLoss = 0\n","    dataLoaderLen = len(dataLoader)\n","    for i, (inputs, targets) in enumerate(dataLoader):\n","        # inputs = inputs.permute(2, 0, 1)\n","        inputs = inputs.permute(0, 3, 1, 2)\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        optimizer.zero_grad()\n","        with torch.set_grad_enabled(True):\n","            outputs = model(inputs)\n","            loss = lossFunction(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","        runningLoss += loss.item() * inputs.size(0)\n","        if(((i*100)//dataLoaderLen) % 10 == 0):\n","            print((i*100//dataLoaderLen), end=\"%,\")\n","    epochLoss = runningLoss / dataLoaderLen\n","    return model, epochLoss\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def ValidateEpoch(model, dataLoader, lossFunction, device):\n","    print(\"Validating Epoch\")\n","    model.train()\n","    runningLoss = 0\n","    dataLoaderLen = len(dataLoader)\n","    for i, (inputs, targets) in enumerate(dataLoader):\n","        inputs = inputs.permute(0, 3, 1, 2)\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        with torch.set_grad_enabled(False):\n","            outputs = model(inputs)\n","            loss = lossFunction(outputs, targets)\n","        runningLoss += loss.item() * inputs.size(0)\n","        if(((i*100)//dataLoaderLen) % 10 == 0):\n","            print((i*100//dataLoaderLen), end=\"%,\")\n","    epochLoss = runningLoss / dataLoaderLen\n","    return epochLoss\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCtFNZeAcKJ-"},"outputs":[],"source":["def DetectImage(model, inputs, device, conf_thres=0.2, iou_thres=0.5):\n","    model.eval()\n","\n","    inputs = torch.tensor(inputs, dtype=torch.float32)\n","    inputs = inputs.unsqueeze(0)\n","    inputs = inputs.permute(0, 3, 1, 2)\n","    inputs = inputs.to(device)\n","    conf_thres = torch.tensor(conf_thres)\n","    with torch.no_grad():\n","        output = model(inputs)\n","        output = output[0]\n","        confidences = output[..., 4:5]\n","        max_confidences, max_indices = torch.max(confidences, dim=1)\n","        box_coordinates = output[..., :4].view(-1, 4)\n","        confidence_scores = output[..., 4].view(-1)\n","        nms_indices = torchvision.ops.nms(box_coordinates, confidence_scores, iou_thres)\n","        # output = output.view(-1, output.shape[-1])[nms_indices]\n","        output = output.view(-1, output.shape[-1])[max_indices]\n","    output = output.squeeze(0)\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vmu96jtn_Xv5"},"outputs":[],"source":["\n","def EvaluateModel(yolov5Model, X_val: list, y_val: list, device ):\n","    randInt = random.randint(0,len(X_val))\n","    image = X_val[randInt]\n","    image1 = deepcopy(image)\n","    predictions = DetectImage(yolov5Model, image, device)\n","    [a1,b1,a2,b2] = y_val[randInt]\n","    bBoxs = []\n","    machingbBoxes = []\n","    albBoxs =[]\n","\n","    for pred in predictions:\n","        x1, y1, x2, y2, m1,m2 = pred[:6]\n","        m1,m2, x1, y1, x2, y2= int(m1), int(m2),int(x1), int(y1), int(x2), int(y2)\n","        if(a1 == x1 or a2 == x2 or b1 == y1 or b2 == y2 ):\n","            if(((x1-x2) >= 17 and (x1-x2) <= 32) and ((y1-y2) >= 31 and (y1-y2)<= 56) ):\n","                machingbBoxes.append([x1, y1, x2,y2])\n","        \n","        if(abs(x1-x2) >= 17  and abs(y1-y2) >= 31 ):\n","            albBoxs.append([x1, y1, x2, y2])\n","\n","        if((abs(x1-x2) >= 17 and abs(x1-x2) <= 32) and (abs(y1-y2) >= 31 and abs(y1-y2)<= 56) ):\n","            bBoxs.append([x1, y1, x2, y2])\n","        \n","        \n","        # x_center, y_center, width, height =x1, y1, x2, y2\n","        # x1 = x_center - (width // 2)\n","        # y1 = y_center - (height // 2)\n","        # x2 = x_center + (width // 2)\n","        # y2 = y_center + (height // 2)\n","\n","    print(\"No. machingbBoxes detected:\" ,len(machingbBoxes) )\n","    print(\"No. albBoxs detected:\" ,len(albBoxs) )\n","    print(\"No. bBoxs detected:\" ,len(bBoxs) )\n","\n","\n","\n","\n","    for bBox in machingbBoxes:\n","        [x1, y1, x2, y2] = bBox\n","        cv2.rectangle(image, (x1, y1), (x2, y2), (0,0,0), 2)\n","\n","    for bBox in albBoxs:\n","        [x1, y1, x2, y2] = bBox\n","        cv2.rectangle(image, (x1, y1), (x2, y2), (0,0,255), 2)\n","\n","    for bBox in bBoxs:\n","        [x1, y1, x2, y2] = bBox\n","        cv2.rectangle(image, (x1, y1), (x2, y2), (255,0,0), 2)\n","\n","    cv2.rectangle(image, (a1, b1), (a2, b2), (0,255,0), 2)\n","\n","\n","    try:\n","        from google.colab.patches import cv2_imshow\n","        cv2_imshow(image)\n","    except:\n","        print(\"using Local\")\n","        cv2.imshow(\"Input Image\", image)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbVy1m6T3nB6"},"outputs":[],"source":["def TrainModel(model, trainDataLoader, valDataLoader, epochs, optimizer, scheduler, lossFunction, device):\n","    for epoch in range(epochs):\n","        startTime = time()\n","        print(\"Epoch {}/{}:\".format(epoch+1, epochs))\n","        startTime = time()\n","        model, trainingEpochLoss = TrainEpoch(model, trainDataLoader, optimizer, lossFunction, device)\n","        validationEpochLoss = ValidateEpoch(model, valDataLoader, lossFunction, device)\n","        scheduler.step(validationEpochLoss)\n","        scheduler.step(trainingEpochLoss)\n","        endTime = time()\n","        timeTaken = endTime - startTime\n","        print()\n","        print(\"Training Loss: {:.4f}\".format(trainingEpochLoss))\n","        print(\"validation Loss: {:.4f}\".format(validationEpochLoss))\n","        print(\"Time taken: {}min, {}, secs\".format(timeTaken//60, int(timeTaken % 60)))\n","    \n","    print(\"Training complete.\")\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4adeyZX3nB6"},"outputs":[],"source":["batchSize = 32\n","inputShape = (640, 640)\n","epochs = 300\n","numAnchors = 3\n","yolo5Version = 'm'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_imN2T83nB6"},"outputs":[],"source":["print(\"Using {} device\".format(device))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5e6cK8HcKJ_"},"outputs":[],"source":["print(\"Downloading Weights of yolo5 Verion \", yolo5Version)\n","weightsURL = \"https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5{}.pt\".format(\n","    yolo5Version)\n","!wget {weightsURL}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z884-9l6bu--"},"outputs":[],"source":["numClasses, X_train, X_val, y_train, y_val = LoadDataSet(\"./DataSet1\")\n","print(numClasses)\n","gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# try:\n","#     from google.colab.patches import cv2_imshow\n","#     cv2_imshow(X_train[10])\n","# except:\n","#     print(\"using Local\")\n","#     cv2.imshow(\"Input Image\", X_train[10])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainDataLoader, valDataLoader = CreateDataLoaders(\n","    X_train, X_val, y_train, y_val, batchSize)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJrRAHOrbu--"},"outputs":[],"source":["yolov5Model = CreateYolov5Model(numClasses, yolo5Version, device)\n","optimizer = optim.Adam(yolov5Model.parameters(), lr=0.01)\n","yolov5LossFunction= SignboardLoss(num_classes = numClasses)\n","yolov5Model = yolov5Model.to(device)\n","yolov5LossFunction = yolov5LossFunction.to(device)\n","scheduler = ReduceLROnPlateau(\n","    optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Atpoji13nB7"},"outputs":[],"source":["trainedModel = TrainModel(yolov5Model, trainDataLoader,valDataLoader, epochs, optimizer, scheduler, yolov5LossFunction, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGDk5r7d3nB7"},"outputs":[],"source":["date = datetime.now()\n","date = date.strftime(\"%m-%d-%H\")\n","torch.save(trainedModel.state_dict(), 'yolov5Modelv4-' + date +'.pth')\n","shutil.copy('/content/yolov5Modelv4-' + date +'.pth', '/content/drive/MyDrive/DL Project/Trained Models/yolov5Modelv4-' + date +'.pth')\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
